---
title: 'Practical Machine Language Project: Write Up'
author: "dahra"
date: "Thursday, August 14, 2015"
output: html_document
---

Background
=================
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. 

In a study, six young health participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E).

Class A corresponds to the specified execution of the exercise, while the other 4 classes correspond to common mistakes. Participants were supervised by an experienced weight lifter to make sure the execution complied to the manner they were supposed to simulate. The exercises were performed by six male participants aged between 20-28 years, with little weight lifting experience. We made sure that all participants could easily simulate the mistakes in a safe and controlled manner by using a relatively light dumbbell (1.25kg).

 (Ref. [1])
 
Objectives
=================
This report shows predictions on the manner in which 6 people performed barbell lifts correctly and incorrectly in 5 different ways.  The data was collected using accelerometers on the belt, forearm, arm, and dumbell.

The 5 different ways the exercises were performed are identified in the training data set as the "classe" variable. The classes are described as follows: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E).

This report describes
-   how the model was built, 
-   how cross validation was used, 
-   the expected out of sample error, 
-   and the reasons choices were made. 

Lastly, the prediction model was used to predict 20 different test cases. 

Method / Expectation 
=========================
####Method of building model/algorithm

Test models with (1) classification/ decision tree algorithm and (2) random forest algorithm. Choose the model with the highest accuracy as the final model.

####Expectation

Expect a better performance using Random Forest.

Single decision trees often have high variance or high bias. Random Forests attempts to mitigate the problems of high variance and high bias by averaging to find a natural balance between the two extremes. (REF[5])

Decision tree learning uses a decision tree as a predictive model which maps observations about an item to conclusions about the item's target value. (REF[4])

vs.

Random forests which operates by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Random forests correct for decision trees' habit of overfitting to their training set. (REF[4])

Data
=========================
Outcome variable : classe
Number predictor variables: 52

###Load & Read Data

```{r, results="hide"}
####Go to working directory
#setwd("PracMachLearning/")

####Download data

dest_train = "train.csv"
dest_test = "test.csv"

if (!file.exists(dest_train)) {
  url_train <- ("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv")
  download.file(url_train,destfile="train.csv")
} else {
  print("test.csv exists")
}

if (!file.exists(dest_test)) {
  url_test <- ("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv")
  download.file(url_test,destfile="test.csv")
} else {
  print("test.csv exists")
}

###Load data into R using read.csv

traindata <- read.csv("train.csv", na.strings=c("", "NA"))
testdata <- read.csv("test.csv", na.strings=c("", "NA"))
```

###Packages & Libraries

```{r}

if (!require("caret")) {
  install.packages("caret", repos="http://cran.rstudio.com/") 
  library("caret")
}


if (!require("e1071")) {
  install.packages("e1071", repos="http://cran.rstudio.com/") 
  library("e1071")
}

if (!require("rpart")) {
  install.packages("rpart", repos="http://cran.rstudio.com/") 
  library("rpart")
}

if (!require("randomForest")) {
  install.packages("randomForest", repos="http://cran.rstudio.com/") 
  library("randomForest")
  
}
```


###Clean Data

Datasets come sometimes with predictors that take an unique value across samples. Such uninformative predictor is more common than you might think. This kind of predictor is not only non-informative, it can break some models you may want to fit to your data (see example below). Even more common is the presence of predictors that are almost constant across samples. One quick and dirty solution is to remove all predictors that satisfy some threshold criterion related to their variance. (Ref. [2])

####Drop Zero Variance Predictor Columns

Used function "nearZeroVar" from the caret package to
  (1.)  remove predictors that have one unique value across samples (zero variance predictors), and
  (2.)  remove predictors that have both 
      (2a) few unique values relative to the number of samples and 
      (2b) large ratio of the frequency of the most common value to the frequency 
         of the second most common value (near-zero variance predictors).


```{r}
#Original number of columns in training and testing datasets
ncol(traindata)
ncol(testdata)

#Zero Variance Predictors - nearZeroVar function

near0 <- nearZeroVar(traindata)
traindata <- traindata[-near0]
testdata <- testdata[-near0]

#New number of columns in training and testing datasets
ncol(traindata)
ncol(testdata)
```

####Remove columns that have mainly missing values

```{r}
removemissingtrain <- apply(traindata,2,function(x) {sum(is.na(x))})
traindata2 <- traindata[,which(removemissingtrain == 0)]

removemissingtest <- apply(testdata,2,function(x) {sum(is.na(x))})
testdata2 <- testdata[,which(removemissingtest == 0)]

#new col count for training
ncol(traindata2)

#old col count for training
ncol(traindata)

#new col count for testing
ncol(testdata2)
#old col count for testing
ncol(testdata)
```

####Remove columns that are not relevant exercise data (drop id, username, windows and time columns)

Col 1 is id, 

Col 2 is user_name , and

Col 3-5 are the timestamps.

Col 6 num_windows 

```{r}
finaltraindata <- traindata2[, -c(1,2,3,4,5,6)]
#New number of columns in training data
ncol(finaltraindata)
finaltestdata <- testdata2[, -c(1,2,3,4,5,6)]
#New number of columns in training data
ncol(finaltestdata)
```

####List of Predictors

```{r, echo=FALSE}
names(finaltraindata)
```

####Plot: frequency of classe in training data

```{r}

plot(finaltraindata$classe , ylim=c(0,6000), col="light blue",ylab="Frequency", xlab="Classe Variable", main="Clean Training Data\nFreq of Classes")

```

Cross validation
================
Sample training data set randomly without replacement 
Training data: 70% of orig Training data
Testing data: 30% of orig Training data
The final test will be against the original testing data set using the best fit model.

### create training and test sets

```{r}
#70% training ; 30% testing
inTrain <- createDataPartition(y=finaltraindata$classe, p=0.7, list=FALSE)
trainset <- finaltraindata[inTrain,]
testset  <- finaltraindata[-inTrain,]

```

### Dimension of Original and Training Dataset

```{r}
rbind("original dataset" = dim(traindata),"training set" = dim(trainset))
```


Predictions
======================

###1. Classificaton Tree Prediction

Recursive partitioning helps us explore the stucture of a set of data, while developing easy to visualize decision rules for predicting a categorical (classification tree) or continuous (regression tree) outcome.

####Grow the tree

```{r}
#Classification Tree
classtree <- rpart(classe ~ ., data=trainset, method="class")
```

#### Determining the Smallest Cross-validated Error 

The following code fragment automatically selects the complexity parameter associated with the smallest cross-validated error.
```{r}
classtree$cptable[which.min(classtree$cptable[,"xerror"]),"CP"]
```

#### Plot of Classification Tree

The basic idea of a classification tree is to first start with all variables in one group; imagine all the points in the above scatter plot. Then find some characteristic that best separates the groups. Then continue this process until the partitions have sufficiently homogeneous or are too small. 
(Ref[3])

```{r}
plot(classtree,
        uniform=TRUE, 
     main="Classification Tree")
text(classtree, cex=.5)



```

#### Prediction01

```{r}

predict01 <- predict(classtree, testset, type = "class")

```

#### confusionMatrix
Cross-tabulation of observed and predicted classes with associated statistics. 
```{r}
confusionMatrix(predict01, testset$classe)
```

###2. Random Forest Prediction
Random forests improve predictive accuracy by generating a large number 
of bootstrapped trees (based on random samples of variables), 
classifying a case using each tree in this new "forest", 
and deciding a final predicted outcome by combining the results 
across all of the trees (an average in regression, a majority vote in classification). 
Breiman and Cutler's random forest approach is implimented via the randomForest package.

```{r}
randforest <- randomForest(classe ~. , data=trainset, method="class")
```

#### Prediction02

```{r}
predict02 <- predict(randforest, testset, type = "class")
```

#### confusionMatrix

Cross-tabulation of observed and predicted classes with associated statistics. 

```{r}
confusionMatrix(predict02, testset$classe)
```

Results
============
As expected, Classification Tree did not improve the performance, so the Random Forest model was used.

Classification Tree Accuracy: 0.7314

Random Forest Accuracy:  0.9939



Submit Random Forest algorithm
=================================
Predicts the outcome levels based on the untouched Testing Dataset using Random Forest prediction algorithm

```{r}
#Prediction for Submission
predictsubmit <- predict(randforest, finaltestdata)

predictsubmit

# Function to Create submission files
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    files = paste0("problem_id_",i,".txt")
    write.table(x[i],file=files,col.names=FALSE,row.names=FALSE,quote=FALSE)
  }
}

pml_write_files(predictsubmit)
```

References
===================

[1] The data for this project come from this source: http://groupware.les.inf.puc-rio.br/har.
[2] http://www.r-bloggers.com/near-zero-variance-predictors-should-we-remove-them/
[3]http://davetang.org/muse/2013/03/12/building-a-classification-tree-in-r/
[4] Wikipedia
[5]http://www.datasciencecentral.com/profiles/blogs/random-forests-algorithm
